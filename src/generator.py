from typing import Dict, Any, List
from pathlib import Path
import re
import os


class ScraperGenerator:
    
    def _sanitize_name(self, name: str) -> str:
        """Convert any string to valid Python identifier."""
        # Remove http/https
        name = re.sub(r'^https?://', '', name)
        
        # Remove domain extensions
        name = re.sub(r'\.(com|org|net|edu|gov|io|co|uk|de|fr|jp|au)$', '', name)
        
        # Replace non-alphanumeric with underscore
        name = re.sub(r'[^a-zA-Z0-9]', '_', name)
        
        # Remove leading/trailing underscores
        name = name.strip('_')
        
        # If starts with digit, prepend 'site_'
        if name and name[0].isdigit():
            name = f'site_{name}'
        
        # Return lowercase, fallback if empty
        return name.lower() if name else 'unknown_site'
    
    def generate_scraper(self, site_name: str, selectors: Dict[str, str], metadata: Dict[str, Any]) -> str:
        """Generate complete Python scraper code as a string."""
        sanitized_name = self._sanitize_name(site_name)
        
        # Extract selectors with safe escaping
        article_links_selector = selectors.get('article_links', 'a')
        title_selector = selectors.get('title', 'h1')
        content_selector = selectors.get('content', 'article')
        
        # Get pre-discovered article URLs if available
        prediscovered_urls = metadata.get('article_urls', [])
        
        # Create the article discovery section based on whether we have prediscovered URLs
        if prediscovered_urls:
            # Use prediscovered URLs (from analysis phase)
            urls_list = ", ".join([f"'{url}'" for url in prediscovered_urls])
            article_discovery_code = f"""        
        # Use pre-discovered article URLs from analysis
        prediscovered_urls = [{urls_list}]
        print(f"ğŸ“„ Using {{len(prediscovered_urls)}} pre-discovered article URLs")
        
        for url in prediscovered_urls:
            article_urls.add(url)"""
        else:
            # Fallback to homepage discovery
            article_discovery_code = f"""
        # Fetch homepage
        response = session.get(homepage_url, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'  # Ensure proper UTF-8 encoding
        homepage_soup = BeautifulSoup(response.text, 'lxml')
        
        # Find article links using detected selector
        print(f"ğŸ” Looking for article links with selector: '{article_links_selector}'")
        article_link_elements = homepage_soup.select('{article_links_selector}')
        
        # Extract URLs from link elements
        for link_element in article_link_elements:
            href = link_element.get('href')
            if href:
                absolute_url = urljoin(homepage_url, href)
                article_urls.add(absolute_url)"""
        
        # Build the code template without f-strings to avoid nesting issues
        code = f"""\"\"\"Auto-generated scraper for {site_name}

Generated by Scraper Generator
This file contains a standalone scraper function that can extract articles
from the website without requiring any external analysis or LLM calls.
\"\"\"

from typing import List
from dataclasses import dataclass
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import sys
import time


@dataclass
class Article:
    \"\"\"Article data structure.\"\"\"
    url: str
    title: str
    content: str


def scrape_{sanitized_name}(homepage_url: str) -> List[Article]:
    \"\"\"Scrape articles from {site_name}.
    
    Args:
        homepage_url: URL of the website homepage
        
    Returns:
        List of Article objects with url, title, and content
    \"\"\"
    articles = []
    article_urls = set()  # Track URLs to avoid duplicates
    
    # Create session with proper headers
    session = requests.Session()
    session.headers.update({{
        'User-Agent': 'Mozilla/5.0 (compatible; ArticleScraper/1.0)'
    }})
    
    try:
        print(f"ğŸ”„ Starting article URL discovery...")
        {article_discovery_code}
        
        print(f"ğŸ“„ Found {{len(article_urls)}} unique article URLs")
        
        if not article_urls:
            print("âš ï¸  No article links found. Check the selector or site structure.")
            return articles
        
        # Scrape each article
        for i, article_url in enumerate(article_urls, 1):
            try:
                print(f"ğŸ“– Scraping article {{i}}/{{len(article_urls)}}: {{article_url}}")
                
                # Fetch article page
                response = session.get(article_url, timeout=10)
                response.raise_for_status()
                response.encoding = 'utf-8'  # Ensure proper UTF-8 encoding
                article_soup = BeautifulSoup(response.text, 'lxml')
                
                # Extract title
                title_element = article_soup.select_one('{title_selector}')
                if not title_element:
                    print(f"   âš ï¸  No title found with selector: '{title_selector}'")
                    continue
                
                title = title_element.get_text(strip=True)
                if not title:
                    print(f"   âš ï¸  Title element found but empty")
                    continue
                
                # Extract content
                content_element = article_soup.select_one('{content_selector}')
                if not content_element:
                    print(f"   âš ï¸  No content found with selector: '{content_selector}'")
                    continue
                
                # Get all paragraph text within content element
                paragraphs = content_element.find_all('p')
                if paragraphs:
                    content = '\\n\\n'.join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
                else:
                    # Fallback to all text if no paragraphs
                    content = content_element.get_text(strip=True)
                
                if not content:
                    print(f"   âš ï¸  Content element found but empty")
                    continue
                
                # Create article object
                article = Article(
                    url=article_url,
                    title=title,
                    content=content
                )
                articles.append(article)
                
                print(f"   âœ… Scraped: '{{title[:60]}}{{\"...\" if len(title) > 60 else \"\"}}' ({{len(content.split())}} words)")
                
                # Be respectful with requests
                time.sleep(0.5)
                
            except Exception as e:
                print(f"   âŒ Error scraping {{article_url}}: {{e}}")
                continue
        
        print(f"\\nğŸ‰ Scraping complete! Found {{len(articles)}} articles.")
        return articles
        
    except Exception as e:
        print(f"âŒ Error fetching homepage: {{e}}")
        return []


if __name__ == "__main__":
    # Test execution
    if len(sys.argv) > 1:
        url = sys.argv[1]
        print(f"Testing scraper with URL: {{url}}")
        results = scrape_{sanitized_name}(url)
        print(f"\\nResults: {{len(results)}} articles scraped")
        
        for i, article in enumerate(results[:3], 1):
            print(f"\\nArticle {{i}}:")
            print(f"  URL: {{article.url}}")
            print(f"  Title: {{article.title}}")
            print(f"  Content length: {{len(article.content)}} characters")
    else:
        print("Usage: python scraper_{sanitized_name}.py <homepage_url>")
"""
        
        return code
    
    def save_scraper(self, site_name: str, code: str) -> str:
        """Save scraper code to file."""
        sanitized_name = self._sanitize_name(site_name)
        filename = f"scraper_{sanitized_name}.py"
        
        # Ensure scrapers directory exists
        scrapers_dir = Path("scrapers")
        scrapers_dir.mkdir(exist_ok=True)
        
        # Write code to file
        file_path = scrapers_dir / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(code)
        
        print(f"ğŸ’¾ Saved scraper to: {file_path}")
        return str(file_path)
    
    def generate_and_save(self, site_name: str, selectors: Dict[str, str], metadata: Dict[str, Any] = None) -> str:
        """Generate and save scraper - convenience method."""
        if metadata is None:
            metadata = {}
            
        print(f"ğŸ› ï¸  Generating scraper for {site_name}...")
        code = self.generate_scraper(site_name, selectors, metadata)
        
        file_path = self.save_scraper(site_name, code)
        print(f"âœ… Scraper generation complete: {file_path}")
        
        return file_path